!pip install nltk
nltk.download('punkt')
nltk.download('stopwords')
!pip install collections
import pandas as pd
from sklearn.cluster import KMeans
import nltk
from nltk.corpus import stopwords
from collections import Counter

# Lê os dados da planilha do Google usando a biblioteca pandas
df = pd.read_csv("https://docs.google.com/spreadsheets/d/14fDe40_aKeosc9UMF0QmwWY9qgaDiEeOI9Ss40MGNiE/export?format=csv")

# Remove os caracteres de formatação de números das colunas 'Taxa de abertura' e 'Taxa de clique (CTR)'
df['Taxa de abertura'] = df['Taxa de abertura'].str.replace(',', '.')
df['Taxa de clique (CTR)'] = df['Taxa de clique (CTR)'].str.replace(',', '.')

# Seleciona as colunas de interesse
data = df[['Taxa de abertura', 'Taxa de clique (CTR)']]

# Converte os valores para float
data = data.astype(float)

# Executa o agrupamento usando o algoritmo K-Means
kmeans = KMeans(n_clusters=3)
kmeans.fit(data)

# Adiciona uma coluna com os rótulos dos clusters ao dataframe
df['cluster'] = kmeans.labels_

# Exibe os resultados
print(df)

# Cria um dicionário que mapeia cada assunto a sua taxa de abertura média
subject_to_avg_open_rate = {}
for subject, group in df.groupby('Assunto'):
  # Tenta converter os valores da coluna 'Taxa de abertura' para float
  group['Taxa de abertura'] = pd.to_numeric(group['Taxa de abertura'], errors='coerce')
  # Remove os valores não numéricos
  group = group.dropna(subset=['Taxa de abertura'])
  # Calcula a média
  avg_open_rate = group['Taxa de abertura'].mean()
  subject_to_avg_open_rate[subject] = avg_open_rate

# Ordena o dicionário pelas taxas de abertura médias
sorted_subjects = sorted(subject_to_avg_open_rate, key=subject_to_avg_open_rate.get, reverse=True)

# Tokeniza os assuntos
subject_tokens = [nltk.word_tokenize(subject) for subject in sorted_subjects]

# Remove as stop words (palavras muito comuns que não acrescentam informação significativa)
filtered_tokens = []
stop_words = set(stopwords.words('portuguese') + ['.', '!', '?', '[', ']', ':'])
for tokens in subject_tokens:
  filtered_tokens.append([token for token in tokens if token.lower() not in stop_words])

# Conta a frequência de cada palavra
counter = Counter()
for tokens in filtered_tokens:
  counter.update(tokens)

# Exibe as palavras-chave mais frequentes
print(f'Palavras-chave mais frequentes: {counter.most_common(10)}')
